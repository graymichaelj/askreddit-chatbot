import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split


with open("post_titles", "rb") as fp:   # Unpickling
    post_titles = pickle.load(fp)

with open("comment_list_0_to_1000", "rb") as fp:   # Unpickling
    comment_list_first_1000 = pickle.load(fp)

with open("coment_list_1001_to_18001", "rb") as fp:   # Unpickling
    comment_list_rest = pickle.load(fp)

with open("post_urls", "rb") as fp:   # Unpickling
    post_urls = pickle.load(fp)

comment_list_subset = comment_list_first_1000+comment_list_rest
post_titles_subset = post_titles[0:18002]

df = pd.DataFrame(np.column_stack(
    [post_titles_subset, comment_list_subset]), columns=['question', 'answer'])

# my scraping sometimes retrieved a top comment which was autogenerated by a moderator bot, need to remove those:
auto_comments = df['answer'].str.startswith('**Attention!')
df = df[~auto_comments]
# sometimes there are also deleted comments which also need to be removed:
deleted_comments = df['answer'].str.startswith('[deleted]')
df = df[~deleted_comments]

train, valtest = train_test_split(df, test_size=0.24)
val, test = train_test_split(valtest, test_size=0.5)
# ^ This results in the test and validation datasets being 12% of the original

train.to_csv('AskRedditTrain.csv')
test.to_csv('AskRedditTest.csv')
val.to_csv('AskRedditVal.csv')
